image:
  repository: "us-docker.pkg.dev/castai-hub/library/vllm-openai"
  tag: "v0.11.2"

replicaCount: 1
resources: {}

modelDownloader:
  image:
    repository: "us-docker.pkg.dev/castai-hub/library/model-downloader"
    tag: "v0.0.8"
  resources:
    requests:
      memory: "500Mi"
      cpu: "100m"
    limits:
      memory: "500Mi"

useRunAiStreamer: false

model:
  # -- HF Model name or path in object storage
  name:
  # -- Optional override for the served model name. If not set, defaults to model.name
  servedName:
  sourceRegistry: "hf"
  registry:
    secretName:
    createSecret: true
    gcs:
      credentialsJsonBase64:
    hf:
      token:
    s3:
      accessKeyId:
      secretAccessKey:
      region:
      endpointUrl:
  cache:
    enabled: false
    bucket: ""
    bucketType: "gcs"
    copyJob:
      backoffLimit: 3
      ttlSecondsAfterFinished: 0
    secretName:
    createSecret: true
    gcs:
      credentialsJsonBase64:
    s3:
      accessKeyId:
      secretAccessKey:
      region:
      endpointUrl:

loraAdapter:
  # -- HF Adapter name or path in object storage
  name:
  sourceRegistry: "hf"
  registry:
    secretName:
    createSecret: true
    gcs:
      credentialsJsonBase64:
    hf:
      token:
    s3:
      accessKeyId:
      secretAccessKey:
      region:
      endpointUrl:

container:
  port: 8000

service:
  type: ClusterIP
  port: 8000

task: "generate"
enableChunkedPrefill: true
maxNumBatchedTokens: 10000
enableAutoToolChoice: false
dtype: "half"
kvCacheDtype: "auto" # Same as default in the vLLM engine. It will read models config and use the same dtype for cache.
maxLoraRank: 128
enableEager: false

# -- Startup probe configuration
startupProbe:
  # -- Enable or disable startup probe
  enabled: true
  initialDelaySeconds: 20
  periodSeconds: 6
  # Allow for up to 60 minutes of startup time
  failureThreshold: 600
  # -- Number of seconds after which the probe times out
  timeoutSeconds: 1
  # -- Minimum consecutive successes for the probe to be considered successful after having failed (must be 1 for startup probe)
  successThreshold: 1
  httpGet:
    path: /health

# -- Readiness probe configuration
readinessProbe:
  # -- Enable or disable readiness probe
  enabled: true
  # -- Number of seconds after the container has started before readiness probe is initiated
  initialDelaySeconds: 5
  # -- How often (in seconds) to perform the readiness probe
  periodSeconds: 5
  # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not ready
  failureThreshold: 3
  # -- Number of seconds after which the probe times out
  timeoutSeconds: 1
  # -- Minimum consecutive successes for the probe to be considered successful after having failed
  successThreshold: 1
   # -- Configuration of the Kubelet http request on the server
  httpGet:
    # -- Path to access on the HTTP server
    path: /health

# -- Liveness probe configuration
livenessProbe:
  # -- Enable or disable liveness probe
  enabled: true
  # -- Number of seconds after the container has started before liveness probe is initiated
  initialDelaySeconds: 15
  # -- Number of times after which if a probe fails in a row, Kubernetes considers that the overall check has failed: the container is not alive
  failureThreshold: 3
  # -- How often (in seconds) to perform the liveness probe
  periodSeconds: 10
  # -- Number of seconds after which the probe times out
  timeoutSeconds: 1
  # -- Minimum consecutive successes for the probe to be considered successful after having failed (must be 1 for liveness probe)
  successThreshold: 1
  # -- Configuration of the Kubelet http request on the server
  httpGet:
    # -- Path to access on the HTTP server
    path: /health

# -- LD_LIBRARY_PATH environment variable for vLLM container. Set to null or empty string to disable.
ldLibraryPath: "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}"

# -- Additional environment variables to set in the vLLM container
env: []
# Example:
# env:
#   - name: MY_CUSTOM_VAR
#     value: "my-value"
#   - name: ANOTHER_VAR
#     value: "another-value"

# -- Extra arbitrary arguments to pass to vLLM
extraArgs: []
# Example:
# extraArgs:
#   - "--max-logprobs=20"
#   - "--disable-sliding-window"

# -- Additional pod labels to set for the vLLM pod
podLabels: {}
# -- Additional annotations labels to set for the vLLM pod
podAnnotations: {}

deployment:
  labels:
  # -- Deployment strategy configuration
  strategy:
    # -- Deployment strategy type (RollingUpdate or Recreate)
    type: RollingUpdate
    # -- Rolling update configuration (only applies when type is RollingUpdate)
    rollingUpdate:
      # -- Maximum number of pods that can be created above desired replicas during update
      maxSurge: 25%
      # -- Maximum number of pods that can be unavailable during update
      maxUnavailable: 25%
