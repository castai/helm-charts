apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "vllm.fullname" . }}
  labels:
    model.aibrix.ai/port: {{ .Values.container.port | quote }}
    {{- include "vllm.labels" . | nindent 4 }}
    {{- with .Values.deployment.labels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:                                                                                                                                  
    matchLabels:
      {{- include "vllm.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "vllm.labels" . | nindent 8 }}
        {{- with .Values.podLabels }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      volumes:
        {{ if .Values.tensorParallelSize }}
        # vLLM needs to access the host's shared memory for tensor parallel inference.
        - name: shm
          emptyDir:
            medium: Memory
            sizeLimit: "10Gi"
        {{ end }}
        {{ if .Values.mountImageCache }}
        - name: llm-cache
          hostPath:
            path: /cache
            type: Directory
        {{ end }}
      containers:
        - name: "vllm"
          image: "{{ required "Required value 'image.repository' must be defined" .Values.image.repository }}:{{ required "Required value 'image.tag' must be defined" .Values.image.tag }}"
          imagePullPolicy: IfNotPresent
          command: ["vllm", "serve"]
          args:
            - {{ .Values.model | quote }}
            - "--port={{ .Values.container.port }}"
            - "--trust-remote-code"
            {{ if .Values.gpuMemoryUtilization }}
            - "--gpu-memory-utilization={{ .Values.gpuMemoryUtilization }}"
            {{ end }}
            - "--task={{ required "Required value 'task' must be defined" .Values.task }}"
            - "--dtype={{ required "Required value 'dtype' must be defined" .Values.dtype }}"
            - "--kv-cache-dtype=fp8_e5m2"
            {{ if .Values.enableChunkedPrefill }}
            - "--enable-chunked-prefill"
            {{ end }}
            {{ if .Values.maxNumBatchedTokens }}
            - "--max-num-batched-tokens={{ .Values.maxNumBatchedTokens }}"
            {{ end }}
            {{ if .Values.enableAutoToolChoice }}
            - "--enable-auto-tool-choice"
            {{ end }}
            {{ if .Values.toolCallParser }}
            - "--tool-call-parser={{ .Values.toolCallParser }}"
            {{ end }}
            - "--download-dir=/cache"
            {{ if .Values.maxModelLen }}
            - "--max-model-len={{ .Values.maxModelLen }}"
            {{ end }}
            {{ if .Values.tensorParallelSize }}
            - "--tensor-parallel-size={{ .Values.tensorParallelSize }}"
            {{ end }}
            {{ if .Values.quantization }}
            - "--quantization={{ .Values.quantization }}"
            {{ end }}
          env:
            - name: LD_LIBRARY_PATH
              value: "/usr/local/nvidia/lib:/usr/local/nvidia/lib64:${LD_LIBRARY_PATH}"
            {{- if .Values.hfToken }}
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: {{ include "vllm.fullname" . }}
                  key: "HF_TOKEN"
            {{- else }}
            {{- if .Values.secretName }}
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: {{ required "secretName must be provided" .Values.secretName }}
                  key: "HF_TOKEN"
            {{- end }}
            {{- end }}
          ports:
            - name: http
              containerPort: {{ .Values.container.port }}
              protocol: TCP
          resources: 
            {{ toYaml .Values.resources | nindent 12 }}
          startupProbe:
            httpGet:
              path: {{ .Values.startupProbe.httpGet.path }}
              port: http
            initialDelaySeconds: {{ .Values.startupProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.startupProbe.periodSeconds }}
            failureThreshold: {{ .Values.startupProbe.failureThreshold }}
          livenessProbe:
            httpGet:
              path: {{ .Values.livenessProbe.httpGet.path }}
              port: http
            initialDelaySeconds: {{ .Values.livenessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.livenessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.livenessProbe.timeoutSeconds }}
            successThreshold: {{ .Values.livenessProbe.successThreshold }}
            failureThreshold: {{ .Values.livenessProbe.failureThreshold }}
          readinessProbe:
            httpGet:
              path: {{ .Values.readinessProbe.httpGet.path }}
              port: http
            initialDelaySeconds: {{ .Values.readinessProbe.initialDelaySeconds }}
            periodSeconds: {{ .Values.readinessProbe.periodSeconds }}
            timeoutSeconds: {{ .Values.readinessProbe.timeoutSeconds }}
            successThreshold: {{ .Values.readinessProbe.successThreshold }}
            failureThreshold: {{ .Values.readinessProbe.failureThreshold }}
          volumeMounts:
          {{ if .Values.tensorParallelSize }}
            - name: shm
              mountPath: /dev/shm
          {{ end }}
          {{ if .Values.mountImageCache }}
            - name: llm-cache
              mountPath: /cache
          {{ end }}
      {{- with .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.tolerations }}
      tolerations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      {{- with .Values.affinity }}
      affinity:
        {{- toYaml . | nindent 8 }}
      {{- end }}
