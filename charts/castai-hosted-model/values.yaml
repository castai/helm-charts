ollama:
  # -- Specifies if Ollama model should be deployed
  enabled: true
vllm:
  # -- Specifies if vLLM model should be deployed
  enabled: false
placementJob:
  # -- Specifies if a node placement job should be deployed
  enabled: false

  # -- Total GPU memory MiB (GPU count * GPU memory MiB) of the node that should be provisioned for this job
  requiredGPUTotalMemoryMiB:

  # -- The names of GPUs that shouldn't be used for this job.
  blacklistedGPUNames: []

  image:
    # -- The image to use for the job
    repository: us-docker.pkg.dev/castai-hub/library/busybox
    # -- The image tag
    tag: "1.37.0"
    # -- Image pull policy
    pullPolicy: IfNotPresent

  # -- Resources for the job
  resources:
    requests:
      nvidia.com/gpu: 1
    limits:
      nvidia.com/gpu: 1
podAutoscaler:
  # -- Specifies if pod autoscaler should be enabled. It is only relevant for vllm deployments
  enabled: false
  # -- Min number of replicas
  minReplicas: 1
  # -- Max number of replicas
  maxReplicas: 3
  # -- The metric to observe for scaling decisions
  targetMetric:
  # -- The threshold value of observed metric to trigger scale up/down decisions
  targetValue:
  # -- Target metric window length
  targetMetricWindow: 30s
  # -- Represents the threshold before scaling up,
  # -- which means no scaling up will occur unless the currentMetricValue exceeds the targetValue by more than upFluctuationTolerance
  upFluctuationTolerance: 0.1
  # -- Represents the threshold before scaling down,
  # -- which means no scaling down will occur unless the currentMetricValue is less than the targetValue by more than downFluctuationTolerance
  downFluctuationTolerance: 0.2
  # -- The name of the vLLM deployment that the pod autoscaler should target
  targetDeploymentName:
